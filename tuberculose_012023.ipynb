{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fbe9f07-14c4-4785-93e7-9f2126930ecd",
   "metadata": {},
   "source": [
    "# Tuberculose A15 Janeiro 2023 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d389d-701c-4983-a737-a8a3f6e50d31",
   "metadata": {},
   "source": [
    "### Reading the XSL files and convert them to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365a2fb-36cd-4edf-be4a-3f7bb19be7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "\n",
    "# Specify the folder path where your .xls files are located\n",
    "folder_path = 'tuberxls'\n",
    "\n",
    "# A list to hold all DataFrames (one per sheet)\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each file in the specified folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is an .xls file\n",
    "    if filename.endswith('.xls'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the file into a DataFrame without headers, assuming the data starts from the first row\n",
    "        df = pd.read_excel(file_path, header=None, engine='xlrd')\n",
    "        \n",
    "        # Directly assign the desired column names\n",
    "        df.columns = [\"Procedimentos realizados\", \"Frequência\"]\n",
    "        \n",
    "        # Drop the first two lines of data after the headers have been set\n",
    "        df = df.drop(index=[0, 1, 2])\n",
    "        \n",
    "        # Reset the index to make sure it starts from 0 after dropping rows\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Append the DataFrame to our list\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Creating DataFrames from the sample data\n",
    "dataframes = [pd.DataFrame(data) for data in dataframes]\n",
    "dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9fbd4a-ee4c-430b-a80c-034ceb1a8427",
   "metadata": {},
   "source": [
    "### Checking one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798acaf-2c19-4247-b086-e3be3624673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e2eba-87f6-4a1e-9d8e-90de3b0ce284",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee1461f-5700-4fd9-ba92-a5a6b1dc74e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in dataframes[5].iterrows():\n",
    "    print(index, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6f579-ee86-4b9a-ac0b-4fb46b1f6ad0",
   "metadata": {},
   "source": [
    "### Creating a dictionary with all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e270a7e-7dd9-4a53-bf7f-eeec1dc14029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brazilian states and Federal District\n",
    "state_names = [\n",
    "    \"Acre\", \"Alagoas\", \"Amazonas\", \"Amapá\", \"Bahia\", \"Ceará\", \"Distrito Federal\",\n",
    "    \"Espírito Santo\", \"Goiás\", \"Maranhão\", \"Minas Gerais\", \"Mato Grosso do Sul\", \"Mato Grosso\",\n",
    "    \"Pará\", \"Paraíba\", \"Pernambuco\", \"Piauí\", \"Paraná\", \"Rio de Janeiro\",\n",
    "    \"Rio Grande do Norte\", \"Rondônia\", \"Roraima\", \"Rio Grande do Sul\", \"Santa Catarina\",\n",
    "    \"São Paulo\", \"Sergipe\", \"Tocantins\"\n",
    "]\n",
    "\n",
    "# Initialize df_states dictionary\n",
    "df_states = {state: {} for state in state_names}\n",
    "\n",
    "for state, df in zip(state_names, dataframes):\n",
    "    df_states[state] = df.to_dict()\n",
    "\n",
    "df_states['Amazonas']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f83fe-66e1-4ecc-9a1e-675e3d286679",
   "metadata": {},
   "source": [
    "### Checking the keys (states) e por Estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d1a858-7c56-44ad-8ecb-509f2ee310d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b74bff2-0cdc-4f4f-b4f8-3a2cf2a7ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states['Amazonas'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9cc1d-d873-4ef0-a139-94f03416309d",
   "metadata": {},
   "source": [
    "### Saving each state dataframe as csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83253e57-f39d-443d-8fad-e3cc5fb7773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in df_states:\n",
    "    # Convert the current state's data to a DataFrame\n",
    "    df = pd.DataFrame(df_states[key])\n",
    "    \n",
    "    # Construct the filename using the state's name\n",
    "    filename = f\"Tuber_{key}_012023.csv\"\n",
    "    \n",
    "    # Save the DataFrame to a CSV file without the index\n",
    "    df.to_csv(f\"csv/{filename}\", index=False)\n",
    "\n",
    "    print(f\"Saved {key} data to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb944511-e427-43d1-bbf0-9eff72bbad65",
   "metadata": {},
   "source": [
    "### Creating a merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a6b18-cb3b-4e6b-a5c2-53772c990b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = {}\n",
    "for state in state_names:\n",
    "    data  = {}\n",
    "    for i in range(len(df_states[state][\"Procedimentos realizados\"])):\n",
    "        data[df_states[state][\"Procedimentos realizados\"][i]] = df_states[state][\"Frequência\"][i]\n",
    "    merged_df[state] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a20e5-6ebf-4025-bab2-139501c52a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.DataFrame(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af48ef-131b-4e46-87ac-93d0ca00e483",
   "metadata": {},
   "source": [
    "### Replacing NaN values for 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f4ec3a-1212-480b-9feb-03f1ea718ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.replace(np.nan, 0)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a56826-fc02-45b4-a808-1b2f8359c8a7",
   "metadata": {},
   "source": [
    "### Removing the rows with only zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea485a55-6a44-4d36-8bb7-a9e65cf9f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum across each row\n",
    "row_sums = merged_df.sum(axis=1)\n",
    "\n",
    "# Use boolean indexing to filter rows where the sum is greater than 0\n",
    "filtered_df = merged_df.loc[row_sums > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910f6f7-7f2e-485f-aa22-6935cc0224d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa0e804-a059-4db5-be89-4f842626c03b",
   "metadata": {},
   "source": [
    "### Dropping the Total row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5388880-486f-46c8-b9e0-f0333e09194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_drop_Total = filtered_df.drop(\"Total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465127a9-2084-46d2-85d9-33e89045eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_drop_Total.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c37fb6b-02e0-4bf9-aa18-cacd4b0cc91e",
   "metadata": {},
   "source": [
    "### Dividing the first column into Code and Description of the procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22163943-6987-4125-8e8e-878888b375c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index to work with it as a regular column\n",
    "filtered_df_drop_Total = filtered_df_drop_Total.reset_index()\n",
    "\n",
    "# Regular expression to separate the numerical part from the text\n",
    "filtered_df_drop_Total[['Code', 'Description']] = filtered_df_drop_Total['index'].str.extract(r'(\\d+)\\s+(.*)')\n",
    "\n",
    "# Drop the original 'index' column if it's no longer needed\n",
    "filtered_df_drop_Total = filtered_df_drop_Total.drop(columns=['index'])\n",
    "\n",
    "# Reorder columns to put 'Code' and 'Description' at the beginning\n",
    "cols = ['Code', 'Description'] + [col for col in filtered_df_drop_Total.columns if col not in ['Code', 'Description']]\n",
    "filtered_df_drop_Total = filtered_df_drop_Total[cols]\n",
    "\n",
    "filtered_df_drop_Total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1313acfc-2f89-4378-a342-624e191fd553",
   "metadata": {},
   "source": [
    "### Creating a total column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f3a5c7-b865-4bf2-8a79-3ae92330ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_drop_Total['Total'] = filtered_df_drop_Total.iloc[:, 2:].sum(axis=1)\n",
    "\n",
    "filtered_df_drop_Total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc57039-9e92-4393-8e5a-5bbeb20cf6a0",
   "metadata": {},
   "source": [
    "### Sorting by the Total column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b098d-8a8b-4df4-a88d-1e34f2e1554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_drop_Total = filtered_df_drop_Total.sort_values(by='Total', ascending=False)\n",
    "filtered_df_drop_Total = filtered_df_drop_Total.drop(\"Total\", axis=1)\n",
    "filtered_df_drop_Total.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b4dda-969a-4c6f-a7a5-4cc41c9cc586",
   "metadata": {},
   "source": [
    "### Saving as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe6206-bf2a-435c-80b0-d86888f7cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_drop_Total.to_csv('csv/tuber_states_012023_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b0628-0d7b-460d-b75e-56ef3228a39f",
   "metadata": {},
   "source": [
    "### Transposing the table to excel for BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c16c4-b04d-473d-98ab-e673bdcf21ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 'Code' and 'Description' columns\n",
    "df_filtered = filtered_df_drop_Total.drop(columns=['Code', 'Description'])\n",
    "\n",
    "# Transposing the dataframe\n",
    "df_transposed = df_filtered.T\n",
    "\n",
    "# Renaming the index and columns for clarity\n",
    "df_transposed.index.name = 'State'\n",
    "df_transposed.columns = [f'Procedure_{i+1}' for i in range(df_transposed.shape[1])]\n",
    "\n",
    "# Summing up the frequencies for each state to get the 'Total Frequency'\n",
    "df_total_frequency = df_transposed.sum(axis=1).reset_index(name='Total Frequency')\n",
    "\n",
    "df_total_frequency.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c731b-3be1-486c-8089-05e49dacae7d",
   "metadata": {},
   "source": [
    "### Save as a xlsx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf71c31-9aaa-4fde-93c2-d4c7a6867773",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total_frequency.to_excel('excel/tuber_states_012023_df.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a840206-5b25-425a-aac1-ff66db30b5a9",
   "metadata": {},
   "source": [
    "### Total of procedures for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d2077-f507-449a-9691-8b88cddd1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_procedures = {}\n",
    "\n",
    "for state in state_names:\n",
    "    total_procedures[state] = filtered_df[state]['Total']\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "total_procedures_df = pd.DataFrame(list(total_procedures.items()), columns=['State', 'Total Procedures'])\n",
    "\n",
    "# Now df is your desired DataFrame\n",
    "total_procedures_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587773ac-1cdf-4038-bb21-68b0e9a26c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_procedures_df.to_csv('csv/tuberculose_012023_total_state.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3dff38-5ea7-46bc-8613-6f771498dfc7",
   "metadata": {},
   "source": [
    "### Total of procedures for each state chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891e0c9-5e44-4d63-8642-e786adfb3a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 10))\n",
    "plt.bar(total_procedures_df['State'], total_procedures_df['Total Procedures'], color='skyblue')\n",
    "\n",
    "plt.title('Total Procedures by State')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Total Procedures')\n",
    "\n",
    "for i, val in enumerate(total_procedures_df['Total Procedures']):\n",
    "    plt.text(i, val + 50, str(val), ha='center')\n",
    "\n",
    "plt.xticks(rotation=45) \n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5ceccd-bdfd-47ff-97c6-8128d1b8c1ff",
   "metadata": {},
   "source": [
    "### Brazil's map for Tuberculose A15 based on the data from 01/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c37a9-896a-4799-80f2-14517303cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brazil's map Data Wragling\n",
    "\n",
    "# Read the shapefile data\n",
    "uf_br = gpd.read_file('map/gadm36_BRA_1.shp')\n",
    "uf_br_geo = uf_br[['NAME_1', 'geometry']]\n",
    "\n",
    "# Rename the column to merge the dataset\n",
    "uf_br_geo.rename(columns={'NAME_1': 'State'}, inplace=True)\n",
    "\n",
    "# Merge the population data with the GeoDataFrame\n",
    "merged_df = uf_br_geo.merge(total_procedures_df, on='State')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4505884-7f97-4123-ad9e-529b87f674d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cc98fa-bfa8-4683-b977-7e0de5a97866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each column (excluding non-numeric columns)\n",
    "for column in merged_df.columns:\n",
    "    if column not in ['State', 'geometry']:\n",
    "        # Normalize population values between 0 and 1 for coloring\n",
    "        col_min = merged_df[column].min()\n",
    "        col_max = merged_df[column].max()\n",
    "        norm = Normalize(vmin=col_min, vmax=col_max)\n",
    "\n",
    "        # Create a scalar mappable to apply colormap to the map\n",
    "        sm = ScalarMappable(cmap='Blues', norm=norm)\n",
    "        sm.set_array([])  # dummy array for the scalar mappable\n",
    "\n",
    "        # Plot the map with the colored regions\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "        merged_df.plot(column=column, cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8')\n",
    "        ax.set_title(f'{column} for Tuberculose A15 in 01/2023')\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        # Loop through the rows of the DataFrame to annotate state abbreviations\n",
    "        for idx, row in merged_df.iterrows():\n",
    "            state_abbr = row['State']  \n",
    "            state_geometry = row['geometry'] \n",
    "            \n",
    "            # Get the centroid of the state geometry\n",
    "            centroid = state_geometry.centroid\n",
    "            \n",
    "            # Annotate the abbreviation at the centroid\n",
    "            ax.annotate(text=state_abbr, xy=(centroid.x, centroid.y), xytext=(3, 3),\n",
    "                        textcoords=\"offset points\", color='black', fontsize=8)\n",
    "\n",
    "\n",
    "        # Create colorbar\n",
    "        cbar = fig.colorbar(sm, ax=ax)\n",
    "        cbar.set_label(column)\n",
    "\n",
    "        # Save or show the map\n",
    "        plt.savefig(f'{column}_012023_map.png')  # Save the figure as an image\n",
    "        plt.show()  # Display the figure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
